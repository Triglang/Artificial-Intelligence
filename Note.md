# Artificial Intelligence

## 知识表示和推理

### 前置知识

1. 项：常量、变量和函数

2. ==**置换**==：置换是形如 $\{x_1/t_1, x_2/t_2, \ldots, x_n/t_n\}$ 的有限集合，其中：

   - $\{x_1, x_2, \ldots, x_n\}$ 是互不相同的变量

   - $\{t_1, t_2, \ldots, t_n\}$ 是项（常量、变量或函数）

   - $x_i/t_i$ 表示用 $t_i$ 置换 $x_i$，其==**合法的前提**==是：

     1. 不允许 $t_i$ 和 $x_i$ 相同

     2. 不允许 $x_i$ 出现在 $t_i$ 中，

     3. ==也不允许出现循环置换==

        如：$\{x /y \} \text{和} \{x/f(y), y/{f(x)}\}$

   注意：置换中的每个变量替换是同时进行的

   > 对于 $P(x, g(y, z))$，做置换 $\{x/y, y/f(a)\}$，
   >
   > 即 $P(x, g(y, z)) \{x/y, y/f(a)\} \Rightarrow P(y, g(f(a), z))$

3. 文字：原子公式及其否定

   - $ P $：正文字；$ \neg P $：负文字

   子句：任何文字的析取。某个文字本身也都是子句。

   - $ P \vee \neg Q $ 记作 $ (P, \neg Q) $
   - 空子句：不包含任何文字的子句，记作 NIL
     - 空子句是永假的，不可满足的。

   子句集：由子句构成的集合（子句的合取）

   - $ (P \vee \neg Q) \wedge (P \vee R) $ 记作 $ \{(P, \neg Q), (P, R)\} $

****

特殊符号

1. `⊢*Am`  或者 ``⊢Am`` ：

   公式序列 A1, A2, ..., Am 称作Am的一个证明，如果 Ai (1 ≤ i ≤ m)：
   - 或者是公理；
   - 或者由Aj1, ..., Ajk (j1, ..., jk < i)用推理规则推得。

   当这样的证明存在时，称Am为系统的定理，记作 `|-*Am`（*是形式系统的名称），或者简记为 |-Am

2. `Γ⊢*Am` 或者 `Γ⊢Am`：

   设 Γ 为一公式集合。公式序列 A1, A2, ..., Am 称作 Am 的以 Γ 为前提的演绎，如果 Ai (1 ≤ i ≤ m)：

   - 或者是 Γ 中的公式
   - 或者是公理
   - 或者由 Aj1, ..., Ajk (j1, ..., jk < i) 用推理规则推得。

   当有这样的演绎时，Am 称作 Γ 的演绎结果，记作 `Γ⊢*Am`（*是形式系统的名称），或者简记为 `Γ⊢Am`，称 Γ 和 Γ 的成员为 Am 的前提

3. $\models_i$ 或者 $ \models $：

   如果推理算法 $ i $ 可以根据 $ KB $ 导出结论 $ \alpha $，则形式化地记为：$ KB \models_i \alpha $

   将 $ S $ 逻辑上蕴含 $ C $ 记为 $ S \models C $

4. $\vdash$：

   记某个永真的子句集合为 $ S $，需要推理得到的子句为 $ C $，基于归结的推理过程从 $ S $ 推导出 $ C $ 记为 $ S \vdash C $

### 归结推理

1. 归结式：对于任意两个子句 $C_1$ 和 $C_2$，若 $C_1$ 中有一个文字 $L$，而 $C_2$ 中有一个与 $L$ 成互补的文字 $\neg L$，则分别从 $C_1$ 和 $C_2$ 中删去 $L$ 和 $\neg L$，并将其剩余部分组成新的析取式。这个新的子句被称为 $C_1$ 和 $C_2$ 关于 $L$ 的归结式，$C_1$ 和 $C_2$ 则是该归结式的亲本子句。

   * 子句 $P$ 和 $\neg P$ 的归结式为空子句
   * 子句 $(W, R, Q)$ 和 $(W, S, \neg R)$ 的归结式为 $(W, Q, S)$

   **定理**：两个子句的归结式是这两个子句集的逻辑推论，如 $\{(P, C_1), (\neg P, C_2)\} \models (C_1, C_2)$

2. 如果 $ S \vdash C $，那么 $ S \models C $

   如果 $ S \vdash NIL $，那么 $ S \models NIL $，反之亦然

3. 鲁滨逊归结原理：检查子句集 S 中是否包含空子句，若包含，则 S 不可满足；若不包含，则在 S 中选择合适的子句进行归结，一旦归结出空子句，就说明 S 是不可满足的

5. 合一：在谓词逻辑的归结过程中，寻找项之间合适的变量置换使表达式一致，这个过程称为合一。

   * 用 $ \sigma = \{x_1/t_1, x_2/t_2, \ldots, x_n/t_n\} $ 来表示任一置换。用 $ \sigma $ 对表达式（语句）$ S $ 作置换后的例简记为 $ S\sigma $。

   * 可以对表达式多次置换：如用 $ \theta $ 和 $ \sigma $ 依次对 $ S $ 进行置换，记为 $ (S\theta)\sigma $。其结果等价于先将这两个置换合成（组合）为一个置换，即 $ \theta\sigma $，再用合成置换对 $ S $ 进行置换，即 $ S(\theta\sigma) $

6. 置换复合的过程：

   设 $ \theta = \{x_1/t_1, x_2/t_2, \ldots, x_n/t_n\} $，$ \sigma = \{y_1/u_1, y_2/u_2, \ldots, y_n/u_n\} $

   1. 构成 $\{x_1/t_1 \sigma, \ldots, x_n/ t_n\sigma, y_1/u_1, \ldots, y_m/u_m\}$；
   2. 如果 $y_j \in (x_1, \ldots, x_n)$，则删除 $y_j/u_j$；
   3. 如果 $t_k \sigma = x_k$，则删除 $x_k / t_k \sigma$;

   > 置换的合成公式比较复杂，不妨看个例子
   >
   > 令 $\theta = \{x / f(y), y / z\}, \sigma = \{x / a, y / b, z / y\}$
   >
   > 步骤1：$\theta \sigma = \{x / f(b), y / y, x / a, y / b, z / y\}$
   >
   > 步骤2：删除 $x / a$ 和 $y / b$
   >
   > 步骤3：删除 $y / y$
   >
   > $\theta \sigma = \{x / f(b), z / y\}$

7. 合一项：对于两个语句 $ f $ 和 $ g $，合一项是使得语句 $ f $ 和 $ g $ 等价的一个置换 $ \sigma $。

   最一般合一项：两个语句 $ f $ 和 $ g $ 的最一般合一项 $ \sigma $ 满足：

   - $ \sigma $ 是 $ f $ 和 $ g $ 的一个合一项
   - 对于 $ f $ 和 $ g $ 的任意其它合一项 $ \theta $，存在一个替换 $ \lambda $ 使得 $ \theta = \sigma \lambda $

8. 求最一般合一项：

   给定两个语句 $ f $ 和 $ g $，

   1. 初始化：$ \sigma = \{\}, S = \{f, g\} $
   2. 如果 $ S $ 包含相同的语句，那么停止算法：当前的置换 $ \sigma $ 为语句 $ f $ 和 $ g $ 的最一般合一项目
   3. 否则，找出 $ S $ 的差异集 $ D = \{e_1, e_2\} $：
      - 若 $ e_1 = v $ 是一个变量且 $ e_2 = t $ 是一个不包含 $ v $ 的项，那么令 $ \sigma = \sigma \cup \{v/t\} $，$ S = S \{v/t\} $。返回步骤 2
      - 否则，停止算法：语句 $ f $ 和 $ g $ 不可合一

9. 谓词公式化为子句集的步骤：

   以将下列谓词公式化为子句集为例：$\forall x \Big( \forall y P(x, y) \rightarrow \neg \forall y \big(Q(x, y) \rightarrow R(x, y)\big) \Big)$

   1. 消去谓词公式中的 “→” 和 “↔”
      $$
      \forall x \Big( \neg \forall y P(x, y) \vee \neg \forall y \big( \neg Q(x, y) \vee R(x, y) \big) \Big) \tag{1}
      $$

   2. 把否定符号移到紧靠谓词的位置上，减少否定符号的辖域。
      $$
      \forall x \Big( \exists y \neg P(x, y) \vee \exists y \big( Q(x, y) \wedge \neg R(x, y) \big) \Big) \tag{2}
      $$

   3. 变量标准化：重新命名变元，使每个量词采用不同的变元，从而使不同量词的约束变元有不同的名字。
      $$
      \forall x \Big( \exists y \neg P(x, y) \vee \exists z \big( Q(x, z) \wedge \neg R(x, z) \big) \Big) \tag{3}
      $$

   4. 消去存在量词

      分两种情况：
      $$
      \begin{cases} 
      \exists x \forall y \Big(\neg P(x, z) \vee R \big(x, y, f(a)\big) \Big) \Rightarrow \forall y \Big(\neg P \big(b, g(y) \big) \vee R\big(b, y, f(a)\big)\Big) \\[2ex]
      \forall x_1 \forall x_2 \ldots \forall x_n \exists y P(x_1, x_2, \ldots, x_n, y) \Rightarrow \forall x_1 \forall x_2 \ldots \forall x_n  P(x_1, x_2, \ldots, x_n, f(x_1, x_2, \dots, x_n))
      \end{cases}
      $$
      原式化为：
      $$
      \forall x \bigg( \neg P\big(x, f(x) \big) \vee \Big(Q\big(x, g(x)\big) \wedge \neg R\big(x, g(x)\big) \Big) \bigg) \tag{4}
      $$
      
   5. 化为前束范式
   
   6. 化为合取范式
      $$
      \forall x \bigg( \Big(\neg P\big(x, f(x)\big) \vee Q\big(x, g(x)\big)\Big) \wedge \Big(\neg P\big(x, f(x)\big) \vee \neg R\big(x, g(x)\big)\Big) \bigg) \tag{5}
      $$
   
   7. 略去全称量词
      $$
      \Big(\neg P\big(x, f(x)\big) \vee Q\big(x, g(x)\big)\Big) \wedge \Big(\neg P\big(x, f(x)\big) \vee \neg R\big(x, g(x)\big)\Big) \tag{6}
      $$
   
   8. 消去合取词，把母式用子句集表示
      $$
      \bigg\{\Big(\neg P\big(x, f(x)\big), Q\big(x, g(x)\big)\Big), \Big(\neg P\big(x, f(x)\big), \neg R\big(x, g(x)\big)\Big)\bigg\} \tag{7}
      $$
   
   9. 子句变量标准化，即使每个子句中的变量符号不同
      $$
      \bigg\{\Big(\neg P\big(x, f(x)\big), Q\big(x, g(x)\big)\Big), \Big(\neg P\big(y, f(y)\big), \neg R\big(y, g(y)\big)\Big)\bigg\} \tag{8}
      $$
   
9. 利用归结反演方法来==**证明定理**==的具体步骤为：

   1. 否定目标公式 $G$，得到 $\neg G$；
   2. 将 $\neg G$ 并入到公式集 $F_1 \wedge F_2 \wedge \ldots \wedge F_n$ 中；
   3. 将公式集化子句集，得到子句集 $S$；
   4. 对 $S$ 进行归结，每次归结的结果并入到 $S$ 中。如此反复，直到得到空子句为止。此时，就证明了在前提 $F_1 \wedge F_2 \wedge \ldots \wedge F_n$ 为真时，结论 $G$ 为真。

10. 利用归结推理**==求解问题==**的具体步骤：

   1. 已知前提 $ F $ 用谓词公式表示，并化为子句集 $ S $；

   2. 把待求解的问题 $ P $ 用谓词公式表示，并否定 $ P $，再与 $ answer $ 构成析取式 $ (\neg P \lor answer) $；

   3. 把 $ (\neg P \lor answer) $ 化为子句集，并入到子句集 $ S $ 中，得到子句集 $ S' $；

   4. 对 $ S' $ 应用归结原理进行归结；

   5. 若得到归结式 $ answer $，则答案就在 $ answer $ 中。

10. 支持集策略：

    - 每次归结时，两个亲本子句中至少要有一个是目标公式否定的子句或其后裔。
    - 支持集 = 目标公式否定的子句集合 $\cup$ 这些子句通过归结生成的所有后裔子句

    特点：

    - 尽量避免在可满足的子句集中做归结，因为从中导不出空子句。而求证公式的前提通常是一致的，所以支持集策略要求归结时从目标公式否定的子句出发进行归结。支持集策略实际是一种目标制导的反向推理。
    - 支持集策略是完备的。

****

#### 归结推理的完备性证明

$\text{S is unsatisfiable} \Rightarrow S \vdash ()$

证明：（数学归纳法）

定义 $P(i)$ 表示：当 S 中含有 $i$ 个 literals 时，$\text{S 不可满足} \Rightarrow S \vdash ()$

1. 当 $i = 1$ 时，P(1) 是成立的。

   当 S 是含有 1 个 literal 的子句集时，不妨设含有的 literal 为 P

   $\because$ S 不可满足

   $\therefore$ S 只可能为 ${(P), (¬P)}$

   $\because$ $(P)$ 与 $(¬P)$ 可归结出 $()$

   $\therefore$ $S \vdash ()$

2. 假设：对于 $1 \le i \le k$，$P(i)$ 都成立

   $\because$ S 是含有 $k+1$ 个文字的子句集合，且 S 不可满足。

   从 S 中选取任意 literal，设选取的 literal 为 P。将 S 分为下列 3 个集合：

   * $S_p$：所有含有 P 的子句集
   * $S_{(\neg P)}$：所有含有 $\neg P$ 的子句集
   * $R$：所有不含 $P$ 或 $\neg P$ 的子句集

   $\therefore$ $R$ 是含有 $k$ 个 literal 的 S 的子句集

   情形 1：若 $R$ 不可满足

   $\because$ $P(k)$ 成立。

   $\therefore R \vdash ()$

   $\because R \subseteq S$

   $\therefore S \vdash ()$，即 $P(k + 1)$ 成立

   情形 2：若 R 可满足，即存在解释 $\widetilde{S}$ 使得 R 中的子句全为真。

   $\because$ S 整体不可满足

   $\therefore S_p, S_{(\neg P)}$ 中存在子句使得任意解释下都不满足

   * 若 $S_p$ 为空集，则取解释 $\widetilde{S}$ 并加上 $P$ 的取值为假，$S$ 便可满足。
   * 若 $S_{(\neg P)}$ 为空集，则取解释 $\widetilde{S}$ 并加上 $P$ 的取值为真，$S$ 便可满足。

   $\therefore S_p, S_{(\neg P)}$ 均不能为空集

   $\therefore$ 存在子句 $(P, \alpha)$ 和 $(\neg P, \beta)$ 使得 $P$ 无论取何值，$S$ 都不可满足。这里 $\alpha$ 和 $\beta$ 表示子句中其它不是 $P$ 的 literal

   $\because$ $(P, \alpha)$ 和 $(\neg P, \beta)$ 不论 $P$ 取何值均不能同时满足

   $\therefore$ 在 $\widetilde{S}$ 解释下，$\alpha$ 为假，$\beta$ 为假

   $\because$ $(P, \alpha)$ 和 $(\neg P, \beta)$ 可归结出 $(\alpha, \beta)$，而 $(\alpha, \beta)$ 在解释 $\widetilde{S}$ 下无法满足。令 $ R' = \{\alpha, \beta\} \cup R $

   $\therefore$ $ R' $ 无法满足，且只含有 $ k $ 个 literal

   $\because$ $ P(k) $ 成立

   $\therefore R' \vdash ( ) $，即 $ R' $ 中存在子句可归结出 ( )

   $\because R'$ 中的子句是属于 $ S $ 或由 $ S $ 中子句归结得到

   $\therefore S \vdash ()$ 即 $ P(k+1) $ 成立

****

#### 易错点

1. 判断对错：如果 $ S \models C $，那么 $ S \vdash C $。（错）

   例如：$ P \models (P, Q) $ 成立，但是 $ P $ 不能归结推导出 $ (P, Q) $

## 搜索技术

搜索的性质：

* 完备性：搜索算法是否总能在问题存在解的情况下找到解
* 最优性：当问题中的动作是需要成本时，搜索算法是否首先找到成本最小的解
* 时间复杂度：搜索算法最多需要探索/生成多少个节点来找到解
* 空间复杂度：搜索算法最多需要将多少个节点储存在内存中

****

### 盲目搜索

定义：这些策略都采用固定的规则来选择下一需要被扩展的状态；这些规则不会随着要搜索解决的问题的变化而变化；这些策略不考虑任何与要解决的问题领域相关的信息

| 标准 | 深度优先   | 宽度优先       | 深度受限   | 迭代加深   | 一致代价           |
| ---- | ---------- | -------------- | ---------- | ---------- | ------------------ |
| 时间 | $ O(b^m) $ | $ O(b^{d+1}) $ | $ O(b^L) $ | $ O(b^d) $ | $ O(b^{C^*/s+1}) $ |
| 空间 | $ O(bm) $  | $ O(b^{d+1}) $ | $ O(bL) $  | $ O(bd) $  | $ O(b^{C^*/s+1}) $ |
| 最优 | 否         | 是             | 否         | 是         | 是                 |
| 完备 | 否         | 是             | 否         | 是         | 是                 |

* $ b $：问题中一个状态最大的后继状态个数
* $ d $：是最短解的动作个数
* $ m $：是遍历过程中最长路径的长度
* $ L $：为限制的搜索深度
* $ C^* $ 为最优解的成本
* $ s $ 为动作的成本下界

****

#### 宽度优先搜索

定义：把当前要扩展的状态的后继状态放在边界的最后

性质：完备性、最优性

- 短的路径会在任何比它长的路径之前被遍历
- 给定路径长度，该长度的路径是有限的
- 最终可以遍历所有长度为 $ d $ 的路径，因此一定可以找出最短的解

时间复杂度：$ 1 + b + b^2 + \ldots + b^d + b(b^d - 1) = O(b^{d+1}) $

空间复杂度：$ b(b^d - 1) = O(b^{d+1}) $

* b = 问题中一个状态最大的后继状态个数，d = 最短解的动作个数

![image-20250401171257141](Note_img/image-20250401171257141.png)

#### 深度优先搜索

定义：把当前要扩展的状态的后继状态放在边界的最前面，边界上总是扩展最深的那个节点

完备性：

- 在状态空间无限的情况下：No
- 在状态空间有限，但是存在无限的路径（例如存在回路）的情况下：No
- 在状态空间有限，且对重复路径进行剪枝的情况下：Yes

最优性：No

时间复杂度：$ 1 + b + b^2 + \ldots + b^m - b = O(b^m) $

* 其中 $ m $ 是遍历过程中最长路径的长度，当 $ m $ 远远大于 $ d $ 时，时间效率会很差；当存在多条解路径的情况下深度优先搜索可能比宽度优先搜索更快找到解

空间复杂度：$ (b-1) + (b-1) + \ldots + (b-1) + b = bm - (m-1) = O(bm) $

* 深度优先回溯点 = 当前路径上的点的未扩展过的兄弟节点，一次只会考虑一条路径，边界上只包含当前探索的最深的节点，以及回溯点
* 线性复杂度是深度优先搜索一个显著的优点

![image-20250401172024336](Note_img/image-20250401172024336.png)

#### 一致代价搜索

定义：边界中，按路径的成本升序排列；总是扩展成本**最低**的那条路径

* 当每种动作的成本是一样的时候，和宽度优先是一样的

性质：完备性、最优性

- 一致代价搜索中，所有成本较低的路径都会在成本高的路径之前被扩展
- 给定成本，该成本的路径数量是有限的
- 成本小于最优路径的路径数量是有限的

时间复杂度和空间复杂度：$ O(b^{C^*/s+1}) $

* $ C^* $：最优解的成本
* 假设每个动作的成本 $ \geq s > 0 $

* 当最优解的路径长度为 $ d $ 时，宽度优先搜索的时间和空间复杂度都是 $ O(b^{d+1}) $。由此类比出一致代价搜索的时空复杂度

#### 深度受限搜索

定义：深度优先搜索，但是预先限制了搜索的深度 $ L $，因此无限长度的路径不会导致深度优先搜索无法停止的问题。但只有当解路径的长度 $ \leq L $ 时，才能找到解

完备性：No

最优性：No

时间复杂度：$ O(b^L) $

空间复杂度：$ O(bL) $

* $ L $ 为限制的最大深度

#### 迭代加深搜索

定义：一开始设置深度限制为 $ L = 0 $，我们迭代地增加深度限制，对于每个深度限制都进行深度受限搜索。如果找到解，或者深度受限搜索没有节点可以扩展的时候可以停止当前迭代，并提高深度限制 $ L $。如果没有节点可以被剪掉（深度限制不能再提高）仍然没有找到解，那么说明已经搜索所有路径，因此这个搜索不存在解

完备性：Yes

最优性：Yes（在每个动作的成本一致的情况下）

* 如果动作成本不一致，则可以使用成本边界代替深度限制 $ L $：

  - 只扩展成本低于成本边界的路径

  - 每次迭代时记录当前还未扩展路径中的最小成本
  - 下一次迭代则提高成本边界

  这样开销会很大，迭代数量为成本数值的构成的集合的大小

时间复杂度：$ (d + 1)b^0 + db + (d - 1)b^2 + \ldots + b^d = O(b^d) $

空间复杂度：$ O(bd) $  

特点：迭代加深搜索可以比宽度优先搜索更高效：不用扩展深度限制上的节点。但是宽度优先搜索需要扩展直到目标节点。

### 启发式搜索

启发式函数 $h(n)$：估计从节点 $n$ 到达目标节点的成本

* 对于所有满足目标条件的节点 $n$，$h(n) = 0$

评价函数 $f(n) = g(n) + h(n)$

* $g(n)$：从初始节点到达节点 $n$ 的路径成本
* $f(n)$ 是经过节点 $n$ 从初始节点到达目标节点的路径成本的估计值

****

#### A 搜索

定义：利用节点对应的 $f(n)$ 值来对边界上的节点进行排序，并总扩展边界中具有最小 $f$ 值的节点。

启发函数的上限问题：

<img src="Note_img/image-20250402003130343.png" alt="image-20250402003130343" style="zoom:50%;" />

* $h(A)$ 的估计太大，甚至超出了实际的cost；过大的启发值淹没实际代价 $g(n)$，使得搜索脱离实际；因此启发式函数应该有上限

令 $ f(n) = g(n) + h(n) $ 和 $ f^*(n) = g^*(n) + h^*(n) $（最优路径代价）
$$
\begin{cases}
f(n) \leq f^*(n) \text{ (使得} n \text{点有被扩展的可能性)} \\
g^*(n) \approx g(n) \text{ (逐步优化得到的)}
\end{cases}
$$
可以得出：$ h(n) \leq h^*(n) $

- $ h(n) = 0 $ 退化为一致代价搜索，则可找到最优解，$ h(n) $ 趋于无穷则算法失效
- 如果 $ h(n) $ 高估了实际成本 $ h^*(n) $：这意味着算法可能会认为通过节点 $ n $ 的路径比实际上更糟，从而可能错过最短路径。因为如果 $ h(n) $ 过大，$ f(n) $ 也会相应变大，导致算法倾向于探索其他看似更有希望（即 $ f $ 值更小）的路径，而这些路径可能并不是最优的。
- 如果 $ h(n) $ 精确或低于实际成本 $ h^*(n) $：这样算法就不会错过任何潜在的最短路径，因为它总是偏向于探索那些估计总成本更低的路径。即使 $ h(n) $ 低估了，最坏的结果就是算法会探索更多节点，这可能会使搜索过程更慢，但仍然可以保证找到最优路径。

#### A*搜索

定义：$h(n)$ 是可采纳的 A 算法

时间复杂度和空间复杂度：$ O(b^{C^*/s+1}) $

* 最坏情况下时空复杂度和一致代价搜索一样，但是高效的启发式函数可以大幅度提升性能

$h(n)$ 的可采纳性：对于所有节点 $n$, 满足 $h(n) \le h^*(n)$，则 $h(n)$ 是可采纳的

$h(n)$ 的一致性（单调性）：对于任意节点 $n_1$ 和 $n_2$，若 $h(n_1) \le c(n_1 \rightarrow n_2) + h(n_2)$，则 $h(n)$ 具有 一致性/单调性

* 对 $h(n_1) \le c(n_1 \rightarrow n_2) + h(n_2)$ 左右两边加 $g(n_1)$，得 $g(n_1) + h(n_1) \le g(n_1) + c(n_1 \rightarrow n_2) + h(n_2) \Rightarrow g(n_1) + h(n_1) \le g(n_2) + h(n_2) \Rightarrow f(n_1) \le f(n_2)$
* 如果变成 $\gt$，意味着高估。因为如果假设 $h(n_2) \approx h^*(n_2)$，则 $h(n_1) \gt c(n_1 \rightarrow n_2) + h^*(n_2) = h^*(n_1)$

可采纳性意味着最优性：最优解一定会在所有成本大于 $C^*$ 的路径之前被扩展到

* $C^*$ 为最优解成本

* 证明：

  > 假设 $ p^* $ 是一个最优解的路径；$ p $ 是一条满足 $ c(p) > c(p^*) $ 的路径，而且路径 $ p $ 在 $ p^* $ 之前被扩展。
  >
  > 那么扩展到路径 $ p $ 时，肯定会有一个 $ p^* $ 上的节点 $ n $ 处在边界上
  >
  > 因为 $ p $ 在 $ p^* $ 之前被扩展，则对于 $ p $ 路径上的最后那个节点（假设为目标节点）$ x $ 满足：$ f(x) \leq f(n) $。
  >
  > 因此 $c(p) = f(x) \leq f(n) = g(n) + h(n) \leq g(n) + h^*(n) = c(p^*)$
  >
  > 和 $ c(p) > c(p^*) $ 相矛盾

* 

满足一致性的启发式函数也一定满足可采纳性

* 证明：

  > Case 1: 从节点 $ n $ 没有路径到达目标节点，则可采纳性一定成立
  >
  > Case 2: 假设 $ n = n_1 \rightarrow n_2 \rightarrow \ldots \rightarrow n_k $ 是从节点 $ n $ 到目标节点的一条最优路径。可以使用数学归纳法证明对于所有的 $ i $，$ h(n_i) \leq h^*(n_i) $。
  >
  > Base: $ h(n_k) = 0 $
  >
  > Induction: $ h(n_{i-1}) \leq c(n_i) + h(n_i) \rightarrow h(n_i) \leq c(n_{i-1} \rightarrow n_i) + h^*(n_i) = h^*(n_{i-1}) $

环检测的影响：如果启发式函数只有可采纳性，不一定能在使用了环检测之后仍保持最优性

<img src="Note_img/image-20250402231122962.png" alt="image-20250402231122962" style="zoom:50%;" />

若启发式函数具备单调性，就能在进行环检测之后仍然保持最优性

* 证明：分为三个命题逐步证明

  命题1：一条路径上的节点的 $f$ 函数值是非递减的

  > $\because h(n_1) \le c(n_1 \rightarrow n_2) + h(n_2)$
  >
  > $\therefore g(n_1) + h(n_1) \le g(n_1) + c(n_1 \rightarrow n_2) + h(n_2)$
  >
  > $\therefore g(n_1) + h(n_1) \le g(n_2) + h(n_2)$
  >
  > $\therefore f(n_1) \le f(n_2)$

  命题2：如果节点 $n_2$​ 在节点 $n_1$ 之后被扩展，则有 $f(n_1) \le f(n_2)$

  命题3：使用单调的启发式函数的 $A^*$ 搜索在第一次扩展到某个状态，就是沿着最小成本的路径进行扩展的

  > 假设路径 $p = n_1 \rightarrow n_2 \ldots \rightarrow n_k = n$ 是第一条被发现的到达 $n$ 的路径
  >
  > 假设路径 $p_j = m_1 \rightarrow m_2 \ldots \rightarrow m_j = n$ 是第二条被发现的到达 $n$ 的路径
  >
  > 假设 $c(p)$ 是通过 $p$ 路径到达 $n$ 节点的成本
  >
  > 假设 $c(p_j)$ 是通过 $p_j$ 路径到达 $n$ 节点的成本
  >
  > 根据命题2，$c(p) + h(n) \leq c(p_j) + h(n)$
  >
  > 因此 $c(p) \leq c(p_j)$

#### IDA*

A*搜索和宽度优先搜索或一致代价搜索一样，也存在潜在的空间复杂度过大的问题。

``IDA*``（迭代加深的 ``A*`` 搜索）：用于解决空间复杂度过大的问题，它类似于迭代加深算法，但是IDA*用于划定界限的不是深度，而是 $ f $ 值，即 $ g + h $ 。

在每次迭代时，IDA* 划定的界限是 $ f $ 值超过上次迭代的界限最少的节点的 $ f $ 值。

## 机器学习

### K-means

**K-means 的算法步骤为：**

1. 选择初始化的 $k$ 个样本作为初始聚类中心 $a = a_1, a_2, \cdots a_k$ ；
2. 针对数据集中每个样本 $x_i$ 计算它到 $k$ 个聚类中心的距离并将其**分到**距离最小的聚类中心所对应的类中；
3. 针对每个类别 $a_j$ ，重新计算它的聚类中心 $a_j = \frac{1}{|c_i|} \sum_{x \in c_i} x$ （即属于该类的所有样本的质心）；
4. 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。

> * 初始质心通常是随机选择的。
>   * 每次运行产生的簇可能会有所不同。
> * 质心（通常）是簇中点的平均值。
> * “接近度”可以通过欧几里得距离、余弦相似度、相关性等来衡量。
> * K-means 对于上述提到的常见相似性度量会收敛。
> * 大多数收敛发生在最初的几次迭代中。
>   * 通常停止条件会改为“直到相对较少的点改变簇”。
> * 复杂度是 $O(nKld)$
>   * n = 点的数量，K = 簇的数量，l = 迭代次数，d = 属性的数量。

**K-means 的评估**

- 最常用的度量是平方误差和（SSE）
  $$
  SSE = \sum_{i = 1}^K \sum_{x \in C_i}dist^2(m_i, x)
  $$

  * $x$ 是簇 $C_i$ 中的一个数据点，$m_i$ 是簇 $C_i$ 的代表点（在上述公式中，$m_i$ 对应于簇的中心（均值））

- 减少SSE的一个简单方法是增加簇的数量 $K$
  - $K$ 值小的好聚类可能比 $K$ 高的差聚类具有更低的 $SSE$

**K 的选择**

- 肘部法则：为每个 *K* 值绘制 SSE 的折线图。如果折线图看起来像一个手臂，那么手臂上的“肘部”就是最佳的 *K* 值。

  ![image-20250509110534649](Note_img/image-20250509110534649.png)

**Bisecting K-means（二分聚类）**

1. 初始化：将所有数据点视为一个簇。
2. 分裂：从当前的簇中选择一个簇进行分裂。选择的标准通常是选择那些分裂后可以最大程度减少误差平方和（SSE）的簇。
3. 应用K-Means：对选定的簇应用标准的K-Means算法，将其分成两个簇。
4. 评估：检查是否达到了所需的聚类数目。如果没有，返回步骤2；如果达到了，算法结束。

**K-means 局限性**

1. 不同大小：

   <img src="Note_img/image-20250509112434211.png" alt="image-20250509112434211" style="zoom:50%;" />

2. 不同密度：

   <img src="Note_img/image-20250509112500166.png" alt="image-20250509112500166" style="zoom:50%;" />

3. 非球形

   <img src="Note_img/image-20250509112516565.png" alt="image-20250509112516565" style="zoom:50%;" />

**克服 K-means 局限性（K-means++）**

- 从**数据集**中随机选择第一个中心点 $x_1$

- 计算每个样本点到已有的最近的聚类中心的距离 $D(x)$，计算每个点被选中的概率
  $$
  \frac{D(x_i)^2}{\sum_{x_k \in X} D(x_k)^2}
  $$
  根据上述概率分布，通过轮盘赌法随机选择下一个中心点。

- 不断重复直到选定 $K$ 个簇

**K-means++ 局限性**

- 需要对数据进行 $K$ 次遍历
- 在大数据应用中，不仅数据量庞大，而且 $K$ 通常也很大。
- 无法扩展

### DBSCAN

在密度的定义下，DBSCAN算法将数据点分为三类：

- **核心点**：如果一个点的eps-邻域内包含至少minPts数目的点，它就是一个核心点。
- **边界点**：如果一个点不是核心点，但在某个核心点的eps-邻域内，则该点是边界点。
- **噪声点**：既不是核心点也不是边界点的点被视为噪声点。

**DBSCAN 算法**：

1. 将所有点标记为核心点、边界点或噪声点。
2. 消除噪声点。
3. 在所有在Eps范围内的核心点之间建立边。
4. 将每组连接的核心点组成一个独立的簇。
5. 将每个边界点分配给其关联的核心点所属的簇之一。

**DBSCAN 优点**

* 抗噪声能力强
* 可以处理不同形状和大小的簇

**DBSCAN 不擅长的情况**

* 不同密度
* 高纬度数据

**DBSCAN 和 K-MEANS的比较**

|                      |            K-means             |                  DBSCAN                  |
| :------------------: | :----------------------------: | :--------------------------------------: |
|       聚类对象       |            所有对象            |                 剔除噪声                 |
|         类型         |         基于原型的聚类         |              基于密度的聚类              |
| 密度差异大时表现不佳 | 难以处理非球形簇和不同大小的簇 | 能够处理不同大小和形状的簇，不受噪声影响 |
|       定义需求       |        需要明确的中心点        |      需要定义密度（eps 和 minPts）       |
|     稀疏高维数据     |            表现良好            |    使用欧几里得距离定义密度时表现较差    |
|     数据分布假设     |        假设球形高斯分布        |     无假设，能处理不同形状和大小的簇     |
|       噪声处理       | 对噪声敏感，可能影响中心点计算 | 能有效处理噪声，边界点和噪声点可明确区分 |
|        簇数量        |            用户定义            |                 自动生成                 |

**评价簇的内部指标：凝聚度和分离度**

* 簇凝聚度：衡量簇内对象的紧密相关程度

  * SSE 组内平方和
    $$
    SSE = \sum_{i} \sum_{x \in C_i} (x - m_i)^2
    $$

* 簇分离度：衡量一个簇与其他簇的区分程度或分离程度

  * SSB：组间平方和
    $$
    SSB = \sum_i |C_i| (m - m_i)^2
    $$
    其中 $∣C_i∣$ 是簇 $i$ 的大小（包含数据点的数量），$m$ 是所有对象的中心（均值）

* $SSE + SSB = SST$（constant）

### 人工神经元模型（MP）

一个典型的神经元模型：包含有 N 个输入，1 个输出，以及 2 个计算功能，每个连接上有一个权值。

神经元的计算公式：用 $\mathbf{x} = [x_1, x_2, x_3]^T $ 表示输入，用 $ W = [w_1, w_2, w_3] $ 表示权值，经过加权计算末端信号为 $ W \cdot \mathbf{x} $，再叠加加非线性函数 $ g $，即是输出值 $ z $

- 在MP模型里，非线性函数 $ g $ 设置是符号函数（sign）
- 由于起到类似神经元的“激活”作用，非线性函数也称作激活函数（Active Function）

<img src="Note_img/image-20250509154942030.png" alt="image-20250509154942030" style="zoom:50%;" />

对神经元模型的图进行简化：将求和函数 sum 与激活函数 sign 合并到一个圆圈里，代表神经元的内部计算。此时，一个“节点”就是一个神经元。

<img src="Note_img/image-20250509155748102.png" alt="image-20250509155748102" style="zoom:50%;" />

1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。然而，MP模型中，权重的值都是预先设置的，因此不能进行学习。

### MLP 模型

#### 感知机模型

感知机：两层神经元构成

* 输入层：在 MP 模型的输入位置添加神经元节点，只传输数据，不做计算
* 输出层：对前面一层的输入进行计算

感知机模型数学化：

- 输入向量 $\mathbf x = [x_1, x_2, x_3]^T $，输出向量 $\mathbf z = [z_1, z_2]^T $，系数矩阵 $ W \in \mathbb{R}^{2 \times 3} $
  $$
  z_1 = g(x_1 w_{1,1} + x_2 w_{1,2} + x_3 w_{1,3}) \\
  z_2 = g(x_1 w_{2,1} + x_2 w_{2,2} + x_3 w_{2,3})
  $$

- 则用矩阵乘法表达感知机的计算公式为：$\mathbf z = g(W \cdot x) $

<img src="Note_img/image-20250510161709170.png" alt="image-20250510161709170" style="zoom:50%;" />

感知机的权重通过训练获得：
$$
\text{对于所有} y \ne \hat y \text{ : } \min_{\mathbf w} - \sum y (\mathbf w \cdot \mathbf x) \\
\hat y = sign(\mathbf w \cdot \mathbf x)
$$

* $\mathbf w$ 是系数矩阵的一行（通俗讲就是：$\mathbf w$ 某一个输出神经元的所有权重）

  <img src="Note_img/image-20250510165823325.png" alt="image-20250510165823325" style="zoom:50%;" />

* $y \ne \hat y$ 代表只需考虑预测错误的的输出神经元

* 对于任一  $y \ne \hat y  \text{, } | \mathbf w \cdot \mathbf x |$ 是该错误输出值相较于正确输出值的误差。而：
  $$
  |\mathbf w \cdot \mathbf x| = - y (\mathbf w \cdot \mathbf x)
  $$

  * 当 $y = +1, \mathbf w \cdot \mathbf x < 0, - y (\mathbf w \cdot \mathbf x) = |\mathbf w \cdot \mathbf x|$
  * 当 $y = -1, \mathbf w \cdot \mathbf x > 0, - y (\mathbf w \cdot \mathbf x) = |\mathbf w \cdot \mathbf x|$

* 感知机的训练就是让误差变得尽可能小。而这里的误差变小可以视作：

  * 若真实值为 $+1$，$\mathbf w$ 和 $\mathbf x$ 的角度尽可能小（钝角的角度变小）
  * 若真是值为 $-1$，$\mathbf w$ 和 $\mathbf x$ 的角度尽可能大（锐角的角度变大）

因此感知机训练法制：
$$
W_t \leftarrow W_t + \Delta W_t \\
\Delta W_t = \eta y \mathbf x
$$

- $ y $：真实的目标
- $ \hat{y} $：感知机的输出
- $ \eta $：学习速率（如 0.1）
- $ x $：训练数据

<img src="Note_img/image-20250510164925232.png" alt="image-20250510164925232" style="zoom:50%;" />

感知机模型的数学几何意义：感知机模型可以试做 $n$ 维空间的决策超平面。

* 对系数矩阵 $W$，该超平面就是 $W \mathbf x = 0$

感知机模型的缺陷：只适用于线性分类任务。

#### 多层感知机（MLP）

多层感知机包含三个层次：一个输入层，一个或多个中间层 (也叫隐藏层，hidden layer) 和一个输出层。输入层与输出层的节点数是固定的，中间层则可以自由指定。

![image-20250510171543280](Note_img/image-20250510171543280.png)

MLP 数学化表达：使用向量和矩阵来表示神经网络中的变量。$ x, a, z $ 是网络中传输的向量数据。$ W_1 $ 和 $ W_2 $ 是网络的矩阵参数。MLP通常还会引入偏置单元，它与后一层的所有节点都有连接。

<img src="Note_img/image-20250510171919527.png" alt="image-20250510171919527" style="zoom: 33%;" />

MLP 模型的数学几何意义：

* 对于两层神经网络，系数矩阵 $W_1$ 和 $W_2$ 的复合会形成一个光滑的曲线（这实际上也是只能做线性分类问题，只不过从平面变成了曲面）。

  <img src="Note_img/image-20250510172318153.png" alt="image-20250510172318153" style="zoom:50%;" />

* 关键在于:  从输入层到隐藏层时，数据在激活函数作用下发生了空间扭曲（这使得数据的原始坐标空间从线性不可分，转换成了线性可分）

  <img src="Note_img/image-20250510172443385.png" alt="image-20250510172443385" style="zoom:60%;" />

训练多层感知机：

* 前向传播：计算当前参数下的误差。

  * 按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

  * 每次计算，都需要经过线性加权求和、激活函数激活两个步骤。

    ![image-20250510173448382](Note_img/image-20250510173448382.png)

* 梯度下降：沿着梯度下降的方向更新参数，使得误差最小。

  每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

  <img src="Note_img/image-20250510174245125.png" alt="image-20250510174245125" style="zoom: 33%;" />

  求导示例：一个具有两个输入、两个隐藏神经元和两个输出神经元的神经网络。考虑 $w_5$，探究 $w_5$ 的变化对总误差的影响有多大

  <img src="Note_img/image-20250510174734161.png" alt="image-20250510174734161" style="zoom: 33%;" />

  * 损失函数：
    $$
    E_{total} = \sum \frac{1}{2} (target_i - out_{oi})^2 \\[4pt]
    $$
    $$
    \begin{aligned}
    \frac{\partial E_{total}}{\partial out_{o1}} &= 2 * \frac{1}{2} (target_{o1} - out_{o1})^{2-1} * (-1) + 0 \\[2pt]
    &= - (target_{o1} - out_{o1}) \\[5pt]
    &= - (0.01 - 0.751365507) = 0.741365507 \\[5pt]
    \end{aligned}
    $$

  * 激活函数：
    $$
    \begin{aligned}
    out_{o1} = \frac{1}{1 + e^{-net_{o1}}} \\[4pt]
    \end{aligned}
    $$
    $$
    \begin{aligned}
    \frac{\partial out_{o1}}{\partial net_{o1}} &= out_{o1}(1 - out_{o1}) = 0.186815602\\[5pt]
    \end{aligned}
    $$

  * 线性求和：
    $$
    net_{o1} &= w_5 * h_1 + w_6 * h_2 + b_2 * 1 \\[6pt]
    $$
    $$
    \begin{aligned}
    \frac{\partial net_{o1}}{\partial w_5} &= 1 * h_1 * w_5^{(1-1)} + 0 + 0 \\
    &= h_1 = 0.5932699992 \\
    \end{aligned}
    $$

  * 参数更新：
    $$
    \text{设置学习率 } \eta = 0.5
    $$
    $$
    \begin{aligned}
    w_5^+ &= w_5 - \eta * \frac{\partial E_{total}}{\partial w_5} \\
    &= 0.4 - 0.5 * 0.082167041 \\[3pt]
    &= 0.35891648
    \end{aligned}
    $$


  继续反向传播 $w_1$：

  <img src="Note_img/image-20250511000903763.png" alt="image-20250511000903763" style="zoom:50%;" />

  <img src="Note_img/image-20250511001226597.png" alt="image-20250511001226597" style="zoom:50%;" />

  <img src="Note_img/image-20250511001303585.png" alt="image-20250511001303585" style="zoom:50%;" />

* 反向传播：根据误差调整参数

  反向传播算法不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

MLP 缺陷：

1. 非凸模型优化问题存在局部最优解问题

2. 梯度衰减问题

   <img src="Note_img/image-20250511001605360.png" alt="image-20250511001605360" style="zoom: 50%;" />

   以 sigmoid 函数为例，$\sigma (x) = \frac{1}{1 + e^{-x}}$，而 $0 \le \frac{d}{dx} \sigma(x) = \sigma (x) \big(1 - \sigma (x)\big) \le 0.25$。在神经网络反向传播梯度时，每传递一层梯度衰减为原来的 0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。

激活函数：

1. **sigmoid**

   数学表达式为：
   $$
   sigmoid(x) = \frac{1}{1 + e^{-x}}
   $$
   求导表达式为：
   $$
   \frac{d(sigmoid(x))}{dx} = sigmoid(x)(1 - sigmoid(x))
   $$
   函数图像为

   ![image-20250511002658927](Note_img/image-20250511002658927.png)

2. **Tanh**

   数学表达式为：
   $$
   Tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
   $$
   函数图像如下：

   ![image-20250511002713085](Note_img/image-20250511002713085.png)

3. **ReLU**

   数学表达式为：
   $$
   ReLU(x) = max(0, x)
   $$
   函数图像如下：

   ![image-20250511002720796](Note_img/image-20250511002720796.png)

   Relu会使部分神经元的输出为0，造成了网络的稀疏性，减少了参数的相互依存关系，缓解过拟合问题的发生

4. **Softmax**

   数学表达式为：
   $$
   Softmax(x) = \frac{e^{x_i}}{\sum_i e^{x_i}}
   $$
   函数图像如下：

   ![image-20250511002728606](Note_img/image-20250511002728606.png)

损失函数：

1. **均方误差（MSE）**
   $$
   MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat y_i)
   $$

2. **交叉熵（LCE）**
   $$
   LCE = -\sum_i^n \sum_j^k y_j^{(i)} \log {\hat y_j^{(i)}}
   $$

   * $y_j^{(i)}$ 是第 $j$ 个输入参数在第 $i$ 个输出的真实值。
   * $\hat y_j^{(i)}$ 是第 $j$ 个输入参数在第 $i$ 个输出的预测值。

分类任务评测指标：

1. **类别级指标**

   - **precision（精确率）**：预测为某类的样本中实际属于该类的比例。

     计算公式：
     $$
     Precision = \frac{TP}{TP + FP} \\
     $$

     * **TP (True Positive)**：正确预测为正类的样本数（预测为A类，实际也是A类）
     * **FP (False Positive)**：错误预测为正类的样本数（预测为A类，实际是其他类）
     * $TP + FP$：预测为正类的总预测数

   - **recall（召回率）**：实际属于某类的样本中被正确预测的比例。

     计算公式：
     $$
     Recall = \frac{TP}{TP + FN}
     $$

     * **TP (True Positive)**：正确预测为正类的样本数（预测为A类，实际也是A类）
     * **FN (False Negative)**：错误预测为负类的样本数（实际是A类，但被预测为其他类）

   - **f1-score**：精确率和召回率的调和平均，综合反映分类质量。

     计算公式：
     $$
     F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
     $$

2. **全局指标**

   - **accuracy**：整体正确分类比例

     计算公式：
     $$
     Accuracy = \frac{正确预测的样本数}{总样本数}
     $$

   - **macro avg**：各类别指标的算术平均

   - **weighted avg**：按样本量加权的平均指标

   由于各类指标同等重要，因此我们选择 accuracy 来作为性能的评价指标。

神经网络的过拟合：

<img src="Note_img/image-20250511003125943.png" alt="image-20250511003125943" style="zoom:33%;" />

## 深度学习

### 卷积神经网络

基于视觉机理的卷积神经网络：

1. 局部特征影响大

   <img src="Note_img/image-20250512092336439.png" alt="image-20250512092336439"  />

2. 重要位置常变化

   <img src="Note_img/image-20250512092415328.png" alt="image-20250512092415328" style="zoom:150%;" />

3. 采样压缩也没差

   <img src="Note_img/image-20250512092503611.png" alt="image-20250512092503611" style="zoom: 70%;" />

卷积核：一个小型的权重矩阵，其数值通过训练自动学习得到。

* 在输入数据上滑动，逐位置进行点乘求和运算，生成特征图。

  <img src="Note_img/image-20250512092807416.png" alt="image-20250512092807416" style="zoom: 67%;" />

特征图维度：
$$
高度~ h = \lfloor \frac{n_h - f + 2p}{s} + 1 \rfloor \\
宽度~ w = \lfloor \frac{n_w - f + 2p}{s} + 1 \rfloor \\
深度~ k = K
$$

* $n_h$：输入图片的高度

* $n_w$：输入图片的宽度

* $K$：卷积核个数

  <img src="Note_img/image-20250512102717326.png" alt="image-20250512102717326" style="zoom:67%;" />

池化：

* 池化操作都有一个固定的窗口，可以称为池化窗口，类似与卷积操作中的卷积核，表示选取的数据范围。

* 最大池化：每次选取移动框内数的最大值

  <img src="Note_img/9a4237da2270164ba32259ec9696d81f.gif" alt="img" style="zoom: 67%;" />

* 池化的作用：有效的缩小参数矩阵的尺寸，从而减少最后连接层的中的参数数量，加快计算速度和防止过拟合。

卷积核的优势：

1. 可以识别局部特征
2. 可以识别不同区域的相似特征
2. 减少参数量
2. 共享权重

### 循环神经网络

独热码：用向量表示单词

* 示例：
  $$
  \text{词典} = \{apple, bag, cat, dog, elephant\} \\
  
  \begin{align*}
  apple &= [1 \ 0 \ 0 \ 0 \ 0] \\
  bag &= [0 \ 1 \ 0 \ 0 \ 0] \\
  cat &= [0 \ 0 \ 1 \ 0 \ 0] \\
  dog &= [0 \ 0 \ 0 \ 1 \ 0] \\
  elephant &= [0 \ 0 \ 0 \ 0 \ 1]
  \end{align*}
  $$

循环神经网络结构：

<img src="Note_img/a226d9719e4d40169f0a9acd037ceb3e.jpeg" alt="img"  />

### 长短期记忆网络

LSTM 网络结构：

<img src="Note_img/image-20250512111631640.png" alt="image-20250512111631640" style="zoom:33%;" />

* $c^{'} = g(z)f(z_i) + cf(z_i)$

LSTM 不易学习，误差表面相当粗糙：

<img src="Note_img/image-20250512111837073.png" alt="image-20250512111837073" style="zoom:50%;" />

* 原因：

  <img src="Note_img/image-20250512112122111.png" alt="image-20250512112122111" style="zoom: 33%;" />

